# OpenSim Analysis Pipeline

This repository is a fork of the [`opencap-processing`](https://github.com/stanfordnmbl/opencap-processing) repository created for the analysis of OpenSim data generated by [OpenCap](https://www.opencap.ai/). The added feature of this repository is the analysis pipeline contained in the `OMA_OpenCapPipeline` folder, which faciliates joint ankle comparisons between OpenSim 3D models and 2D [MoveNet](https://www.tensorflow.org/hub/tutorials/movenet#:~:text=MoveNet%20is%20an%20ultra%20fast,applications%20that%20require%20high%20accuracy.) pose estimation outputs on identical videos.

## Getting Started

### Environment

To create the Conda environment to support this repository, follow the instructions in `README_OpenCap.md` for precise instructions and reasoning behind each command. Below is an over-generalized summary of these command instructions:

* `conda create -n opencap-processing python=3.10`
* `conda activate opencap-processing`
* `conda install -c opensim-org opensim=4.4.1=py310np121`
* `python -m pip install -r requirements.txt`
* `python createAuthenticationEnvFile.py`

Note this repository requires an OpenCap account which can be created [here](https://app.opencap.ai/login).

#### Troubleshooting

After completing these instructions, if this environment fails to work, run `pip install pandas==2.1.4 opencv-python==4.9.0.80` in the `opencap-processing` environment before exploring other possible causes for errors.

### Starter datasets

Our analysis pipeline has been used to analyze data from the `LabValidation_withVideos` dataset found [here](https://simtk.org/frs/?group_id=2385). To get started with some sample analysis, follow the data organization instructions detailed in the `OMA_OpenCapPipeline/dataExploration.ipynb` notebook.

To generate MoveNet outputs on exact videos in a format that fits this pipeline, use the `src/inference/inference_movenet.py` function in the [`oma-ml` repsitory](https://github.com/openmotionai/oma-ml).

To further understand the data organization conventions used by this pipeline, see the data loading cells in the `OMA_OpenCapPipeline/dataExploration.ipynb` notebook. Also, for an example `Data` folder containing OpenSim data from OpenCap's open source `LabValidation_withVideos` dataset and MoveNet outputs on videos from this dataset, see the `Data` folder in the `OpenCap ML Project` OneDrive folder.

## Analysis and Metrics

Once the environment and `OMA_OpenCapPipeline/dataExploration.ipynb` notebook is working properly with organized local data, streamlined analysis can be performed using the `OMA_OpenCapPipeline/processing.py` script. This file can generate relative error and absolute error data organized by movement type and camera angle, and store results locally. These results can be viewed and analyzed using the `OMA_OpenCapPipeline/errorVisualization.ipynb` notebook.

Additionally, MoveNet coordinate smoothing techniques can have a profound impact on MoveNet joint angle plots. To experiment with different smoothing methods, the `OMA_OpenCapPipeline/movenetSmoothing.ipynb` notebook comtains a rough sample workflow with different smoothing functions and an output cell.

## Time Synchronization Across Modalities

A notable challenge taken on by this pipeline is time synchronization betwen OpenSim and MoveNet outputs during analysis. We have implemented a relatively suitable custom algorithm present in both `OMA_OpenCapPipeline/dataExploration.ipynb` and `OMA_OpenCapPipeline/processing.py`. If the user plans on using this pipeline for advanced analysis, we highly recommend that the user familiarize themselves with this algorithm.
